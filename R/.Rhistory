}
# ll.removed.outliers = ll.output[-ll.outliers,]
ll.outlier.data = ll.numeric[ll.outliers,]
ll.removed.outliers = ll.numeric[-ll.outliers,]
write.csv(ll.removed.outliers, file = "ll_transformed.csv", row.names = F)
write.csv(ll.outlier.data, file = "ll_outliers.csv", row.names = F)
dim(ll.outlier.data)
dim(ll.removed.outliers)
dim(ll.numeric)
4544+2919
dim(LLathena)
cp.numeric = CPathena[, -c(1,2)]
ll.numeric = LLathena[, -c(1,2)]
dim(ll.numeric)
dim(LLathena)
dim(CPathena)
dim(cp.numeric)
dim(ll.count)
dim(LLcount)
dim(LLanomaly)
dim(ll.numeric)
dim(landline.data)
dim(LLcount)
dim(LLanomaly)
dim(LLathena)
dim(ll.numeric)
dim(ll.mads)
# Strip off the categorial data and the cellphone numbers
cp.numeric = CPathena[, -c(1,2)]
ll.numeric = LLathena[, -c(1,2)]
### CELLPHONE
# Compute the Median Absolute Deviation (mad)
cp.mads = apply(cp.numeric, 2, mad)
ll.mads = apply(ll.numeric, 2, mad)
cp.y = names(cp.numeric[cp.mads == 0])
ll.y = names(ll.numeric[ll.mads == 0])
# If MAD=0, then var=0, so we remove this useless feature
cp.novariance = c()
ll.novariance = c()
for(i in 1:length(cp.mads)) {
if(cp.mads[i] == 0) {
cp.novariance = append(cp.novariance, i)
}
if(ll.mads[i] == 0) {
ll.novariance = append(ll.novariance, i)
}
}
# Remove columns with no variance
cp.numeric = cp.numeric[, -cp.novariance]
ll.numeric = cp.numeric[, -ll.novariance]
# Re-compute the MAD for trimmed data
cp.mads = apply(cp.numeric, 2, mad)
ll.mads = apply(ll.numeric, 2, mad)
### Cellphone data
# detect outliers using a modified-z-score approach
cp.output = data.frame(cp.numeric) # Copy the data for rewriting
for(i in 1:ncol(cp.numeric)) {
cp.col = cp.numeric[,i]
cp.col.med = median(cp.col)
cp.col.mad = cp.mads[i]
for(j in 1:nrow(cp.numeric)) {
cp.x = cp.numeric[j,i]
cp.output[j,i] = (0.6745 * (cp.x - cp.col.med)) / cp.col.mad
}
}
# Remove rows that contain outliers
cp.outliers = c()
for(i in 1:nrow(cp.output)) {
for(j in 1:ncol(cp.output)) {
if(abs(cp.output[i,j] > 3.5)) {
cp.outliers = append(cp.outliers, i)
}
}
}
# cp.removed.outliers = cp.output[-cp.outliers,]
cp.outlier.data = cp.numeric[cp.outliers,]
cp.removed.outliers = cp.numeric[-cp.outliers,]
write.csv(cp.removed.outliers, file = "cp_transformed.csv", row.names = F)
write.csv(cp.outlier.data, file = "cp_outliers.csv", row.names = F)
### LandLine data
# detect outliers using a modified-z-score approach
ll.output = data.frame(ll.numeric) # Copy the data for rewriting
for(i in 1:ncol(ll.numeric)) {
ll.col = ll.numeric[,i]
ll.col.med = median(ll.col)
ll.col.mad = ll.mads[i]
for(j in 1:nrow(ll.numeric)) {
ll.x = ll.numeric[j,i]
ll.output[j,i] = (0.6745 * (ll.x - ll.col.med)) / ll.col.mad
}
}
# Remove rows that contain outliers
ll.outliers = c()
for(i in 1:nrow(ll.output)) {
for(j in 1:ncol(ll.output)) {
if(abs(ll.output[i,j] > 3.5)) {
ll.outliers = append(ll.outliers, i)
}
}
}
# ll.removed.outliers = ll.output[-ll.outliers,]
ll.outlier.data = ll.numeric[ll.outliers,]
ll.removed.outliers = ll.numeric[-ll.outliers,]
write.csv(ll.removed.outliers, file = "ll_transformed.csv", row.names = F)
write.csv(ll.outlier.data, file = "ll_outliers.csv", row.names = F)
dim(ll.mads)
length(ll.mads)
length(ll.y)
length(cp.mads)
# Strip off the categorial data and the cellphone numbers
cp.numeric = CPathena[, -c(1,2)]
ll.numeric = LLathena[, -c(1,2)]
### CELLPHONE
# Compute the Median Absolute Deviation (mad)
cp.mads = apply(cp.numeric, 2, mad)
ll.mads = apply(ll.numeric, 2, mad)
cp.y = names(cp.numeric[cp.mads == 0])
ll.y = names(ll.numeric[ll.mads == 0])
# If MAD=0, then var=0, so we remove this useless feature
cp.novariance = c()
ll.novariance = c()
for(i in 1:length(cp.mads)) {
if(cp.mads[i] == 0) {
cp.novariance = append(cp.novariance, i)
}
}
for(i in 1:length(ll.mads)) {
if(ll.mads[i] == 0) {
ll.novariance = append(ll.novariance, i)
}
}
# Remove columns with no variance
cp.numeric = cp.numeric[, -cp.novariance]
ll.numeric = ll.numeric[, -ll.novariance]
# Re-compute the MAD for trimmed data
cp.mads = apply(cp.numeric, 2, mad)
ll.mads = apply(ll.numeric, 2, mad)
### Cellphone data
# detect outliers using a modified-z-score approach
cp.output = data.frame(cp.numeric) # Copy the data for rewriting
for(i in 1:ncol(cp.numeric)) {
cp.col = cp.numeric[,i]
cp.col.med = median(cp.col)
cp.col.mad = cp.mads[i]
for(j in 1:nrow(cp.numeric)) {
cp.x = cp.numeric[j,i]
cp.output[j,i] = (0.6745 * (cp.x - cp.col.med)) / cp.col.mad
}
}
# Remove rows that contain outliers
cp.outliers = c()
for(i in 1:nrow(cp.output)) {
for(j in 1:ncol(cp.output)) {
if(abs(cp.output[i,j] > 3.5)) {
cp.outliers = append(cp.outliers, i)
}
}
}
# cp.removed.outliers = cp.output[-cp.outliers,]
cp.outlier.data = cp.numeric[cp.outliers,]
cp.removed.outliers = cp.numeric[-cp.outliers,]
write.csv(cp.removed.outliers, file = "cp_transformed.csv", row.names = F)
write.csv(cp.outlier.data, file = "cp_outliers.csv", row.names = F)
### LandLine data
# detect outliers using a modified-z-score approach
ll.output = data.frame(ll.numeric) # Copy the data for rewriting
for(i in 1:ncol(ll.numeric)) {
ll.col = ll.numeric[,i]
ll.col.med = median(ll.col)
ll.col.mad = ll.mads[i]
for(j in 1:nrow(ll.numeric)) {
ll.x = ll.numeric[j,i]
ll.output[j,i] = (0.6745 * (ll.x - ll.col.med)) / ll.col.mad
}
}
# Remove rows that contain outliers
ll.outliers = c()
for(i in 1:nrow(ll.output)) {
for(j in 1:ncol(ll.output)) {
if(abs(ll.output[i,j] > 3.5)) {
ll.outliers = append(ll.outliers, i)
}
}
}
# ll.removed.outliers = ll.output[-ll.outliers,]
ll.outlier.data = ll.numeric[ll.outliers,]
ll.removed.outliers = ll.numeric[-ll.outliers,]
write.csv(ll.removed.outliers, file = "ll_transformed.csv", row.names = F)
write.csv(ll.outlier.data, file = "ll_outliers.csv", row.names = F)
dim(ll.outlier.data)
dim(ll.removed.outliers)
ll.numeric = LLathena[, -c(1,2)]
dim(ll.numeric)
length(ll.novariance)
length(ll.mads)
length(ll.y)
dim(ll.outliers)
length(ll.outliers)
ll.outliers
distinct(ll.outliers)
unique(ll.outliers)
# Strip off the categorial data and the cellphone numbers
cp.numeric = CPathena[, -c(1,2)]
ll.numeric = LLathena[, -c(1,2)]
### CELLPHONE
# Compute the Median Absolute Deviation (mad)
cp.mads = apply(cp.numeric, 2, mad)
ll.mads = apply(ll.numeric, 2, mad)
cp.y = names(cp.numeric[cp.mads == 0])
ll.y = names(ll.numeric[ll.mads == 0])
# If MAD=0, then var=0, so we remove this useless feature
cp.novariance = c()
ll.novariance = c()
for(i in 1:length(cp.mads)) {
if(cp.mads[i] == 0) {
cp.novariance = append(cp.novariance, i)
}
}
for(i in 1:length(ll.mads)) {
if(ll.mads[i] == 0) {
ll.novariance = append(ll.novariance, i)
}
}
# Remove columns with no variance
cp.numeric = cp.numeric[, -cp.novariance]
ll.numeric = ll.numeric[, -ll.novariance]
# Re-compute the MAD for trimmed data
cp.mads = apply(cp.numeric, 2, mad)
ll.mads = apply(ll.numeric, 2, mad)
### Cellphone data
# detect outliers using a modified-z-score approach
cp.output = data.frame(cp.numeric) # Copy the data for rewriting
for(i in 1:ncol(cp.numeric)) {
cp.col = cp.numeric[,i]
cp.col.med = median(cp.col)
cp.col.mad = cp.mads[i]
for(j in 1:nrow(cp.numeric)) {
cp.x = cp.numeric[j,i]
cp.output[j,i] = (0.6745 * (cp.x - cp.col.med)) / cp.col.mad
}
}
# Remove rows that contain outliers
cp.outliers = c()
for(i in 1:nrow(cp.output)) {
for(j in 1:ncol(cp.output)) {
if(abs(cp.output[i,j] > 3.5)) {
cp.outliers = append(cp.outliers, i)
}
}
}
# cp.removed.outliers = cp.output[-cp.outliers,]
cp.outlier.data = cp.numeric[cp.outliers,]
cp.removed.outliers = cp.numeric[-cp.outliers,]
write.csv(cp.removed.outliers, file = "cp_transformed.csv", row.names = F)
write.csv(cp.outlier.data, file = "cp_outliers.csv", row.names = F)
### LandLine data
# detect outliers using a modified-z-score approach
ll.output = data.frame(ll.numeric) # Copy the data for rewriting
for(i in 1:ncol(ll.numeric)) {
ll.col = ll.numeric[,i]
ll.col.med = median(ll.col)
ll.col.mad = ll.mads[i]
for(j in 1:nrow(ll.numeric)) {
ll.x = ll.numeric[j,i]
ll.output[j,i] = (0.6745 * (ll.x - ll.col.med)) / ll.col.mad
}
}
# Remove rows that contain outliers
ll.outliers = c()
for(i in 1:nrow(ll.output)) {
for(j in 1:ncol(ll.output)) {
if(abs(ll.output[i,j] > 3.5)) {
ll.outliers = append(ll.outliers, i)
}
}
}
ll.outliers = unique(ll.outliers)
# ll.removed.outliers = ll.output[-ll.outliers,]
ll.outlier.data = ll.numeric[ll.outliers,]
ll.removed.outliers = ll.numeric[-ll.outliers,]
dim(ll.outlier.data)
dim(ll.removed.outliers)
136+142
cp.numeric = CPathena[, -c(1,2)]
ll.numeric = LLathena[, -c(1,2)]
### CELLPHONE
# Compute the Median Absolute Deviation (mad)
cp.mads = apply(cp.numeric, 2, mad)
ll.mads = apply(ll.numeric, 2, mad)
cp.y = names(cp.numeric[cp.mads == 0])
ll.y = names(ll.numeric[ll.mads == 0])
# If MAD=0, then var=0, so we remove this useless feature
cp.novariance = c()
ll.novariance = c()
for(i in 1:length(cp.mads)) {
if(cp.mads[i] == 0) {
cp.novariance = append(cp.novariance, i)
}
}
for(i in 1:length(ll.mads)) {
if(ll.mads[i] == 0) {
ll.novariance = append(ll.novariance, i)
}
}
# Remove columns with no variance
cp.numeric = cp.numeric[, -cp.novariance]
ll.numeric = ll.numeric[, -ll.novariance]
# Re-compute the MAD for trimmed data
cp.mads = apply(cp.numeric, 2, mad)
ll.mads = apply(ll.numeric, 2, mad)
### Cellphone data
# detect outliers using a modified-z-score approach
cp.output = data.frame(cp.numeric) # Copy the data for rewriting
for(i in 1:ncol(cp.numeric)) {
cp.col = cp.numeric[,i]
cp.col.med = median(cp.col)
cp.col.mad = cp.mads[i]
for(j in 1:nrow(cp.numeric)) {
cp.x = cp.numeric[j,i]
cp.output[j,i] = (0.6745 * (cp.x - cp.col.med)) / cp.col.mad
}
}
# Remove rows that contain outliers
cp.outliers = c()
for(i in 1:nrow(cp.output)) {
for(j in 1:ncol(cp.output)) {
if(abs(cp.output[i,j] > 3.5)) {
cp.outliers = append(cp.outliers, i)
}
}
}
cp.outliers = unique(cp.outliers)
# cp.removed.outliers = cp.output[-cp.outliers,]
cp.outlier.data = cp.numeric[cp.outliers,]
cp.removed.outliers = cp.numeric[-cp.outliers,]
write.csv(cp.removed.outliers, file = "cp_transformed.csv", row.names = F)
write.csv(cp.outlier.data, file = "cp_outliers.csv", row.names = F)
dim(cp.outlier.data)
dim(cp.removed.outliers)
# Strip off the categorial data and the cellphone numbers
cp.numeric = CPathena[, -c(1,2)]
dim(cp.numeric)
2499+2614
cp.numeric = CPathena[, -c(1,2)]
ll.numeric = LLathena[, -c(1,2)]
### CELLPHONE
# Compute the Median Absolute Deviation (mad)
cp.mads = apply(cp.numeric, 2, mad)
ll.mads = apply(ll.numeric, 2, mad)
cp.y = names(cp.numeric[cp.mads == 0])
ll.y = names(ll.numeric[ll.mads == 0])
# If MAD=0, then var=0, so we remove this useless feature
cp.novariance = c()
ll.novariance = c()
for(i in 1:length(cp.mads)) {
if(cp.mads[i] == 0) {
cp.novariance = append(cp.novariance, i)
}
}
for(i in 1:length(ll.mads)) {
if(ll.mads[i] == 0) {
ll.novariance = append(ll.novariance, i)
}
}
# Remove columns with no variance
cp.numeric = cp.numeric[, -cp.novariance]
ll.numeric = ll.numeric[, -ll.novariance]
# Re-compute the MAD for trimmed data
cp.mads = apply(cp.numeric, 2, mad)
ll.mads = apply(ll.numeric, 2, mad)
### Cellphone data
# detect outliers using a modified-z-score approach
cp.output = data.frame(cp.numeric) # Copy the data for rewriting
for(i in 1:ncol(cp.numeric)) {
cp.col = cp.numeric[,i]
cp.col.med = median(cp.col)
cp.col.mad = cp.mads[i]
for(j in 1:nrow(cp.numeric)) {
cp.x = cp.numeric[j,i]
cp.output[j,i] = (0.6745 * (cp.x - cp.col.med)) / cp.col.mad
}
}
# Remove rows that contain outliers
cp.outliers = c()
for(i in 1:nrow(cp.output)) {
for(j in 1:ncol(cp.output)) {
if(abs(cp.output[i,j] > 3.5)) {
cp.outliers = append(cp.outliers, i)
}
}
}
cp.outliers = unique(cp.outliers)
# cp.removed.outliers = cp.output[-cp.outliers,]
cp.outlier.data = cp.numeric[cp.outliers,]
cp.removed.outliers = cp.numeric[-cp.outliers,]
write.csv(cp.removed.outliers, file = "cp_transformed.csv", row.names = F)
write.csv(cp.outlier.data, file = "cp_outliers.csv", row.names = F)
### LandLine data
# detect outliers using a modified-z-score approach
ll.output = data.frame(ll.numeric) # Copy the data for rewriting
for(i in 1:ncol(ll.numeric)) {
ll.col = ll.numeric[,i]
ll.col.med = median(ll.col)
ll.col.mad = ll.mads[i]
for(j in 1:nrow(ll.numeric)) {
ll.x = ll.numeric[j,i]
ll.output[j,i] = (0.6745 * (ll.x - ll.col.med)) / ll.col.mad
}
}
# Remove rows that contain outliers
ll.outliers = c()
for(i in 1:nrow(ll.output)) {
for(j in 1:ncol(ll.output)) {
if(abs(ll.output[i,j] > 3.5)) {
ll.outliers = append(ll.outliers, i)
}
}
}
ll.outliers = unique(ll.outliers)
# ll.removed.outliers = ll.output[-ll.outliers,]
ll.outlier.data = ll.numeric[ll.outliers,]
ll.removed.outliers = ll.numeric[-ll.outliers,]
write.csv(ll.removed.outliers, file = "ll_transformed.csv", row.names = F)
write.csv(ll.outlier.data, file = "ll_outliers.csv", row.names = F)
#Produce histograms
for(i in 1:ncol(cp.removed.outliers)) {
hist(cp.removed.outliers[,i], freq = F, breaks = 30, main = names(cp.removed.outliers)[i])
}
for(i in 1:ncol(ll.removed.outliers)) {
hist(ll.removed.outliers[,i], freq = F, breaks = 30, main = names(ll.removed.outliers)[i])
}
dim(ll.removed.outliers)
dim(cp.removed.outliers)
names(cp.removed.outliers)
names(ll.removed.outliers)
cp.numeric = CPathena[, -c(1,2)]
ll.numeric = LLathena[, -c(1,2)]
sapply(ll.numeric, var)
sapply(ll.numeric, mad)
sapply(ll.numeric, mad) > 0
which(sapply(ll.numeric, mad) > 0)
which(sapply(cp.numeric, mad) > 0)
dim(cp.removed.outliers)
for(i in 1:ncol(ll.removed.outliers)) {
hist(ll.removed.outliers[,i], freq = T, breaks = 30, main = names(ll.removed.outliers)[i])
}
install.packages('princomp')
install.packages('prcomp')
install.packages('asdfasdfasdfsdf')
?prcomp
library(devtools)
install.packages('devtools')
library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)
mtcars.pca <- prcomp(mtcars[,c(1:7,10,11)], center = TRUE,scale. = TRUE)
ggbiplot(mtcars.pca)
?prcomp
ll.pca = prcomp(ll.removed.outliers, center = TRUE, scale. = TRUE)
ggbitplot(ll.pca)
ggbiplot(ll.pca)
cp.pca = prcomp(cp.removed.outliers, center = TRUE, scale. = TRUE)
ggbiplot(cp.pca)
?ggbiplot
ggbiplot(cp.pca, labels.size = 1)
ggbiplot(cp.pca, varname.size = 1)
ggbiplot(cp.pca, varname.size = 5)
?prcomp
biplot(cp.pca)
cp.pca
ggbiplot(cp.pca, varname.size = 2)
ggbiplot(cp.pca, varname.size = 4)
ggbiplot(cp.pca, varname.size = 1)
?ggbiplot
ggbiplot(cp.pca, varname.size = 1, varname.adjust = 3)
ggbiplot(cp.pca, varname.size = 3, varname.adjust = 3)
ggbiplot(cp.pca, varname.size = 3, varname.adjust = 5)
ggbiplot(cp.pca, varname.size = 3, ellipse = T, circle = T, groups =  c("Total_BPeakHour_Calls", "Total_B30Day_Calls"))
ggbiplot(cp.pca, varname.size = 3, ellipse = T, groups =  c("Total_BPeakHour_Calls", "Total_B30Day_Calls"))
ggbiplot(cp.pca, varname.adjust = 8, ellipse = T, groups =  c("Total_BPeakHour_Calls", "Total_B30Day_Calls"))
ggbiplot(cp.pca, varname.adjust = 7, ellipse = T, groups =  c("Total_BPeakHour_Calls", "Total_B30Day_Calls"))
ggbiplot(cp.pca, varname.adjust = 6, ellipse = T, groups =  c("Total_BPeakHour_Calls", "Total_B30Day_Calls"))
?prcomp
ll.pca
ggbiplot(ll.pca)
?princomp
ggbiplot(princomp(ll.removed.outliers))
ll.pca = prcomp(ll.removed.outliers, center = T, scale. = T)
?prcomp
ll.pca$x
cp.pca = prcomp(cp.removed.outliers, center = T, scale. = T)
ll.pca = prcomp(ll.removed.outliers, center = T, scale. = T)
ggbiplot(cp.pca)
ggbiplot(ll.pca)
write.csv(cp.pca$x, file = "cp_pca.csv", row.names = F)
write.csv(ll.pca$x, file = "ll_pca.csv", row.names = F)
cp.pca
summary(cp.pca)
summary(ll.pca)
ggbiplot(cp.pca, choices = c(3,4))
ggbiplot(cp.pca, choices = c(1,2,3,4))
ggbiplot(cp.pca, choices = c(1,4))
ll.pca1 = princomp(ll.removed.outliers)
ggbiplot(ll.pca1)
cp.pca = prcomp(cp.removed.outliers, center = T, scale. = T)
ll.pca = prcomp(ll.removed.outliers, center = T, scale. = T)
ggbiplot(cp.pca, choices = c(1,4))
ggbiplot(ll.pca)
write.csv(cp.pca$x, file = "cp_pca.csv", row.names = F)
write.csv(ll.pca$x, file = "ll_pca.csv", row.names = F)
summary(ll.pca)
summary(cp.pca)
